# ğŸ“ˆ RL-Driven Stock Trading Agent using Deep Q-Learning

A production-grade reinforcement learning system applying **Deep Q-Networks (DQN)** to historical SPY (S&P 500 ETF) data. This project showcases a custom OpenAI Gym environment, an end-to-end training pipeline, and thoughtful metrics tracking.

## ğŸ’¼ Purpose

This project demonstrates:

- Custom environment engineering for financial markets.
- Design, training, and evaluation of a Deep Q-Learning agent.
- Realistic reward shaping using profit and Sharpe ratio.
- Modular, reusable code structure with configuration via YAML.
- A reproducible pipeline ready for experimentation and extension.

## ğŸ¤“ Problem Overview

The agent learns to **buy, sell, or hold** SPY based on historical price features to maximise long-term return while controlling drawdown and volatility.

- **Action space**: buy, sell, or hold.
- **State space**: price indicators (close, SMA, RSI, etc.).
- **Reward**: profit and loss (PnL) penalised by volatility and over-trading.

## ğŸª’ Repository Structure

```
rl_learning_stockmarket/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Hyper-parameters for training
â”œâ”€â”€ data/
â”‚   â””â”€â”€ spy.csv              # Historical SPY data (provided separately)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py             # Training script
â”‚   â”œâ”€â”€ eval.py              # Evaluation and back-testing
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â””â”€â”€ stock_env.py     # Custom Gym environment
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ dqn.py           # DQN agent definition
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py        # Logging utilities
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ equity_curve.png     # Saved equity curve
â”‚   â””â”€â”€ metrics.csv          # Logged evaluation metrics
â”œâ”€â”€ Dockerfile               # Container for reproducibility
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project overview (this file)
```

## âš™ï¸ Setup

Clone the repository and install dependencies:

```bash
git clone https://github.com/edawite/rl_learning_stockmarket.git
cd rl_learning_stockmarket
pip install -r requirements.txt
```

## ğŸš€ Quickstart

Train the agent using the configuration file:

```bash
python src/train.py --config config/config.yaml
```

Evaluate a trained policy:

```bash
python src/eval.py --model_path results/best_model.pth
```


## ğŸ” Future Improvements

- Add PPO, SAC, and DDPG agents for comparison.
- Incorporate LSTM or transformer models for richer state encoding.
- Integrate a broker API for live paper-trading.
- Extend the feature set with additional technical indicators.

## ğŸ¤“ Credits

Developed by Edjutawee Dawit. This repository is designed for internship-ready demonstration of reinforcement learning applied to algorithmic trading.

## ğŸ” Legal

**Disclaimer**: This project is for research and educational purposes only and does not constitute financial advice.
